# Investigating-the-Approximation-Power-of-Neural-Networks

Neural networks provide a computational model for solving various types of problems. They are able to solve complex equations, such as differential equations, therefore, they could be widely applied in various field, for instance, computational fluid dynamics. Nevertheless, for traditional neural network methods, an unique network is required for a different equation and its slight modification, which is inefficient and inconvenient. Besides, neural networks are also suitable for approximating unknown functions with less model parameters required compared with traditional finite element method. Based on the theoretical research, the proof has been deduced that any function could be approximated with any desired accuracy. Plenty of experiments regarding to various types of
network activation functions are implemented to gain approximation results to verify the theoretical arguments. However, the arguments related to ReLU network have not been verified yet.

Therefore, this project aims to deal with two parts: 
(1) Investigate the computational power of neural networks by solving parametric ordinary differential equations with sigmoid activation function.
(2) Investigate the approximation ability of neural networks by performing experiments to verify the theoretical arguments of ReLU network. 

For the first part, satisfying results are acquired through experiments. In the second part, we emphasize the gap between experimental results and the theoretical arguments.
